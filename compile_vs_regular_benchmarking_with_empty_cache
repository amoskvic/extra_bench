# Copyright (c) 2023, Tri Dao, Albert Gu.

import argparse
import time
import json

import torch
import torch.nn.functional as F

from einops import rearrange

from transformers import AutoTokenizer, AutoModelForCausalLM

from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel

parser = argparse.ArgumentParser(description="Generation benchmarking")
parser.add_argument("--model-name", type=str, default="state-spaces/mamba-2.8b")
parser.add_argument("--prompt", type=str, default=None)
#parser.add_argument("--promptlen", type=int, default=256) # was 2048
#parser.add_argument("--genlen", type=int, default=128)
parser.add_argument("--temperature", type=float, default=0.7)
parser.add_argument("--topk", type=int, default=1)
parser.add_argument("--topp", type=float, default=0.9)
parser.add_argument("--minp", type=float, default=0.0)
parser.add_argument("--repetition-penalty", type=float, default=1.2)
#parser.add_argument("--batch", type=int, default=2) # was 32
parser.add_argument("--dtype", type=str, default='float16')
#parser.add_argument("--perf_log", type=str, default='perf_log')
#parser.add_argument("--do_compile", type=str, default=True)
parser.add_argument("--warmup_steps", type=int, default=12)
parser.add_argument("--repeats", type=int, default=10)
args = parser.parse_args()

import pandas as pd


device = "cuda:5"
dtype = torch.float16

# if args.dtype.lower() == 'float16':
#     args.dtype = torch.float16
# elif args.dtype.lower() == 'bfloat16':
#     args.dtype = torch.bfloat16
# elif args.dtype.lower() == 'float32':
#     args.dtype = torch.float32


print(f"Loading model Mamba")

tokenizer = AutoTokenizer.from_pretrained("EleutherAI/gpt-neox-20b")
model = MambaLMHeadModel.from_pretrained(args.model_name, device=device, dtype=dtype)

model.eval()

# model_cur = torch.compile(model, mode="max-autotune")

# print("Compiled the model")


all_res = {"compiled": [], "batch_size": [], "gen_len": [], "prompt_len": [], "time": []}

for compiled in [False, True]:
    #for prompt_len in [2**11]:
    for prompt_len in [128, 2048, 4096, 8192]:
        #for gen_len in [128, 2048, 4096]:
        for gen_len in [1]:
        #for gen_len in [128, 256, 512, 1024, 2048]:
            for batch_size in [2, 64]:
                
                print(f"Benchmarking for {'compiled' if compiled else 'uncompiled'} prompt length {prompt_len}, batch size {batch_size}")
                
                input_ids = torch.randint(1, 1000, (batch_size, prompt_len), dtype=torch.long, device=device)
                attn_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)

                max_length = input_ids.shape[1] + gen_len
                cg = False

                torch.random.manual_seed(0)

                with torch.no_grad():

                    

                    if compiled:
                        model_cur = torch.compile(model, mode="max-autotune")
                    else:
                        model_cur = model

                    fn = lambda: model_cur.generate(
                            input_ids=input_ids,
                            max_length=max_length,
                            cg=cg,
                            return_dict_in_generate=True,
                            output_scores=True,
                            enable_timing=True,
                            temperature=args.temperature,
                            top_k=args.topk,
                            top_p=args.topp,
                            min_p=args.minp,
                            repetition_penalty=args.repetition_penalty,
                        )


                    for i in range(args.warmup_steps):
                        out = fn()


                    for i in range(args.repeats):
                        torch.cuda.empty_cache()

                        start = torch.cuda.Event(enable_timing=True)
                        end = torch.cuda.Event(enable_timing=True)
                        # profile
                        torch.cuda.synchronize()

                        start.record()
                        out = fn()
                        torch.cuda.synchronize()
                        #torch.cuda.Event.synchronize()
                        end.synchronize()
                        end.record()
                        torch.cuda.synchronize()

                        result = start.elapsed_time(end) # / args.repeats

                        #print(f"Time spent: in {'compiled' if compiled else 'uncompiled'} regime: {result}")

                        all_res["compiled"].append(compiled)
                        all_res["batch_size"].append(batch_size)
                        all_res["prompt_len"].append(prompt_len)
                        all_res["gen_len"].append(gen_len)
                        all_res["time"].append(result)
                    
df_res = pd.DataFrame.from_dict(all_res)
df_res.to_csv("benchmarking_results_run_large_ttft_v3_rerun_on_cuda5_extra_cache_clearing.csv")